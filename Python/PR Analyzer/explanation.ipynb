{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <u> Rotten Scripts Repository PR Analysis </u>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "## *This document shows the statistial analysis of the repo \"[Rotten Scripts](https://github.com/HarshCasper/Rotten-Scripts)\".*\r\n",
    "### *Check out the repo *[here](https://github.com/HarshCasper/Rotten-Scripts)*.*\r\n",
    "\r\n",
    "## *Check out the code [[here](https://github.com/Aditya-Komaravolu/PR-Analyzer/blob/main/PRScrapper.ipynb)].*\r\n",
    "\r\n",
    "\r\n",
    " \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1: Visit the target website\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "* ### First of all, the websites that we visit show interative data( or technically called as static data/display data).<b>\r\n",
    "\r\n",
    "* ### Since the data is available and encoded with markup language, it can be retrieved and stored to make exploration analysis.<b>\r\n",
    "\r\n",
    "\r\n",
    "* ### This itself is called Web Scraping,i.e, getting out all the necessary information from a website.<b>\r\n",
    "<img src=https://user-images.githubusercontent.com/64011471/131397259-3eb7ffd1-b2c8-4cc2-915e-1415758c4a84.png width=1100 height=650>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2: Locate the PR markup using Debugger \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "* ## To enable the debugger, use `Fn + F12` key. \r\n",
    "* ## Trace out the location of the each PR container class manually.\r\n",
    "* ## Here, as you can see the `class=\"Box-row Box-row--focus-gray p-0 mt-0 js-navigation-item js-issue-row\"` is the required target class and the hieararchy is as follows:\r\n",
    "    * ### `application-main`\r\n",
    "      * ### `clearfix new-discussion-timeline container-xl px-3 px-md-4 px-lg-5`\r\n",
    "        * ### `repository-content `\r\n",
    "          * ### `js-check-all-container`\r\n",
    "            * ### `....`\r\n",
    "              * ### `....`\r\n",
    "                * ### `Box-row Box-row--focus-gray p-0 mt-0 js-navigation-item js-issue-row navigation-focus`\r\n",
    " \r\n",
    "  \r\n",
    "<img src=https://user-images.githubusercontent.com/64011471/131397987-63ad1aa6-8edc-4ad6-bcf5-51117a39cbaa.png width=900 height=500>\r\n",
    "\r\n",
    "* ## Once we locate the required class for the PR containers, then we can use a simple for loop to extract the `<a>` tag which contains the actual reference link to the PR location.\r\n",
    "* ## Later we combine `href` along with `base_url: https://github.com/` to get the complete website link for each pull request.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 3: PR status\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "  * ## GitHub has three states for any Pull Request namely, ***Open***, ***Closed*** and ***Merged***.\r\n",
    "  * ## We have over 690+ PR's and hence is it very important to know the status of each of them.\r\n",
    "  * ## Best way is to repeat step 2 and find the html tag which is associated with the Status data. Once you find out, we can use `BeautifulSoup` to fetch all the required tags with class as the condidtion, so that we get the status info.\r\n",
    "  \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 4: Create a Pandas DataFrame\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "  * ## Now that we have `PR Links` and `Status` ready, it is time to combine them and store them together as a DataFrame (in simple terms, a table).\r\n",
    "  ![image](https://user-images.githubusercontent.com/64011471/131401426-ee775ce1-3a4f-4473-b2bb-c0695cf2d318.png)\r\n",
    "  \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 5: Fetch all the Contributors for each PR\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "  * ## We need the names of the contributors to better understand the diversity of the repo.\r\n",
    "  * ## For that, we can again follow step 2 and get the required parameters.\r\n",
    "  * ## Use `BeautifulSoup` to get the required tags containing the data of the contributors.\r\n",
    "  * ## Later update the DataFrame by adding a new column to represent the Contributors.\r\n",
    "  \r\n",
    "  ![image](https://user-images.githubusercontent.com/64011471/131402166-78c4e372-e4da-40e7-91ba-a8a89107156f.png)\r\n",
    "\r\n",
    "  \r\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 6: Distinct Contributors along with Count\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "  * ## We need to find out all the distinct contributors where every PR can involve more than one contributor.\r\n",
    "  * ## For that we can write a nested for loop to append only if the string is different from all the strings which were added to the list in before iterations.To refer codebase, click [[here](https://github.com/Aditya-Komaravolu/PR-Analyzer/blob/main/PRScrapper.ipynb)].\r\n",
    "  * ## Once done, we now have to compute the total number of contributions were done by each contributor in all the PR's. \r\n",
    "  * ## To calculate the count, define a for loop which tallies each and every contributor with the the actual column in the DataFrame.\r\n",
    "  \r\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 7: Create a JSON File\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "  * ## WE can store the contributors and the count as key-value pairs in a dictionary.\r\n",
    "  * ## Later use file pointer and json package to dump the dictionary as a json object.\r\n",
    " \r\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Step 8: Visualizing Data\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "  * ## Now that we have PR status and Total number of contributions by each contributor, we can plot them for better visualization of the entire repo.\r\n",
    "  * ## prefer `Bar Plots` for the current situation because we have categorical variables to represent and what else is better than a bar graph!  :)\r\n",
    "\r\n",
    "  * ## PR status:\r\n",
    "  \r\n",
    "    ![image](https://user-images.githubusercontent.com/64011471/131405268-1ff889ba-81a5-46e5-b37e-d727df5bed62.png)\r\n",
    "  \r\n",
    "  * ## Top 5 Contributors:\r\n",
    "  \r\n",
    "    ![image](https://user-images.githubusercontent.com/64011471/131405338-d630d7fa-6ac9-4bc5-b98d-dd4f768de181.png)\r\n",
    "\r\n",
    "\r\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <u>Additional Analysis</u>: Finding out the Average time taken for each Pull Request\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " \r\n",
    " * ## To find out the start and end time, locate the time reference for every container in the DevTools.\r\n",
    "  \r\n",
    "  <img src=https://user-images.githubusercontent.com/64011471/131406319-db2f7d6a-9467-4bf2-8e5b-24773ed79c10.png width=1140 height=650>\r\n",
    " \r\n",
    " * ## Once we get the tag associated with the datetime, we can reapeat step 2 to get complete dates along with timestamp from every PR.\r\n",
    " \r\n",
    " ![image](https://user-images.githubusercontent.com/64011471/131406692-f3e690cf-0e3b-4971-9d22-9b1947806081.png)\r\n",
    "\r\n",
    "\r\n",
    " \r\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    " * ## Take minimum and maximum values from all the timestamps from every PR and subtract them to get the time taken to merge a PR.\r\n",
    " * ## If you want, you can convert it into Days H:M:S format by using the datetime package to convert the string obejct to a datetime object.\r\n",
    " * ## Datetime objects can be converted into seconds format by using `total_seconds`.\r\n",
    " \r\n",
    " ![image](https://user-images.githubusercontent.com/64011471/131408733-dc0eec0b-6843-41e1-8b79-50a2e42a69c4.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Categorizing the Active PR time using KMeans Clustering\r\n",
    " \r\n",
    " \r\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*  ### Now that we have the active PR time in seconds format, it is now easier to fit the data to model.\r\n",
    " * ### I choose KMeans because here we have active PR time in the form of clusters (atleast as per my initial observation) and since we are about to deal with uncategorised Pr time values, what's better than KMeans.\r\n",
    " * ### Pass the single-column 'Active PR Time(in Seconds)' to the model with `n_clusters=3`.\r\n",
    " * ### Later plot the centriods of those clusters and those values are itself the 'Average PR time' values.\r\n",
    " \r\n",
    " ![image](https://user-images.githubusercontent.com/64011471/131408811-0887cb50-5fcd-4b77-b853-2df2e85e66a0.png)\r\n",
    " \r\n",
    " * ### These three values describe the whole dataset where the average PR active period is: \r\n",
    "   *  #### Either approx 3 days.\r\n",
    "   *  #### Or approx 28 days (a month).\r\n",
    "   * #### Or approx 80 days or greater."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Please see the code [[here]()] to get better insights and clarity.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}