import requests
import csv
import time
from bs4 import BeautifulSoup


def write_to_csv(row_array):
    """
    The function stores the scraped info in a .csv file
    :param row_array:
    :return:
    """
    # Headings for the first row of the file
    header_list = ["Title", "Author", "Date and Time", "Upvotes", "Comments", "Url"]
    file_name = input("\nEnter the name of file to store the info: ")

    # Adding info into the rows of the file
    with open(file_name + ".csv", "a", encoding="utf-8") as csv_f:
        csv_pointer = csv.writer(csv_f, delimiter=",")
        csv_pointer.writerow(header_list)
        csv_pointer.writerows(row_array)

    print(f"Done! Check your directory for {file_name}.csv file!")


def scraper():
    """
    The function scrapes the post info from the desired subreddit and stores it
    into the desired file.
    :return:
    """
    subreddit = input("Enter the name of the subreddit: r/").lower()
    max_count = int(input("Enter the maximum number of entries to collect: "))

    # Generating the URL leading to the desired subreddit
    url = "https://old.reddit.com/r/" + subreddit

    # Using a user-agent to mimic browser activity
    headers = {"User-Agent": "Mozilla/5.0"}

    req = requests.get(url, headers=headers)

    if req.status_code == 200:
        # Parsing through the web page for obtaining the right html tags and
        # scraping the details required
        soup = BeautifulSoup(req.text, "html.parser")
        print("\nCOLLECTING INFORMATION....")

        attrs = {"class": "thing"}
        counter = 1
        full = 0
        reddit_info = []
        while 1:
            for post in soup.find_all("div", attrs=attrs):
                try:
                    title = post.find("a", class_="title").text

                    author = post.find("a", class_="author").text

                    time_stamp = post.time.attrs["title"]

                    comments = post.find("a", class_="comments").text.split()[0]
                    if comments == "comment":
                        comments = 0

                    upvotes = post.find("div", class_="score likes").text
                    if upvotes == "â€¢":
                        upvotes = "None"

                    link = post.find("a", class_="title")["href"]
                    link = "www.reddit.com" + link

                    # Storing the scraped data in an array
                    reddit_info.append(
                        [title, author, time_stamp, upvotes, comments, link]
                    )

                    if counter == max_count:
                        full = 1
                        break

                    counter += 1
                except AttributeError:
                    continue

            if full:
                break

            try:
                # To go to the next page
                next_button = soup.find("span", class_="next-button")
                next_page_link = next_button.find("a").attrs["href"]

                time.sleep(2)

                req = requests.get(next_page_link, headers=headers)
                soup = BeautifulSoup(req.text, "html.parser")
            except:
                break

        # Writing the stored information in a .csv file
        print("DONE!\n")
        write_to_csv(reddit_info)

    else:
        print("Error fetching results.. Try again!")


if __name__ == "__main__":
    scraper()
